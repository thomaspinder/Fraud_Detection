{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import scikitplot as skplt\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.combine import SMOTETomek \n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Reader function\n",
    "This function drops the ordered_product_key and the campaign_key columns from a given file as whilst needed when creating the dataset for joins.etc, they serve no use when trying to model fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_reader(file):\n",
    "    data_read = pd.read_csv(file)\n",
    "    data = data_read.drop([\"Ordered_Product_Key\", \"Campaign_Key\"], 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NaN function\n",
    "Naturally. it is impossibly to model fraud if no fraud actually exists in a specific site. For a given dataframe, this function returns the number of NaN values within a dataset as well as the number of total number of fraudulent observations, if any at all. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_checker(dataframe):\n",
    "    num_nan = np.sum(dataframe.isnull().sum())\n",
    "    num_fraud = dataframe['fraud_status'].sum()\n",
    "    return num_nan, num_fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SMOTE function\n",
    "This function applies Tomek line and SMOTE to the dataset, allowing for fraud to be better modelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_smote(feature, label):\n",
    "    print(\"Raw Data: \" + str(sorted(Counter(label).items())))\n",
    "    smt = SMOTETomek(random_state=42)\n",
    "    feature_resampled, label_resampled = smt.fit_sample(feature, label)\n",
    "    print(\"Resampled: \" + str(sorted(Counter(label_resampled).items())))\n",
    "    return feature_resampled, label_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the cross-validation scores\n",
    "For each of the 6 sites, this function plots the cross-validation scores vs. the number of trees. Additionally, this saves each graph as a .png file within a local directory. This function was used when determining the number of trees needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotter(scores, array1, array2, tree_list, directory, file_write):\n",
    "    plt.plot(tree_list, scores)\n",
    "    plt.plot(tree_list, array1 + array2, 'b--')\n",
    "    plt.plot(tree_list, array1 - array2, 'b--')\n",
    "    plt.ylabel('CV score')\n",
    "    plt.xlabel('# of trees')\n",
    "    plt.savefig(directory + 'accuracy_plots/dataset' + str(file_write).strip(file_dir) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Metric function\n",
    "This function calculates and returns the recall, precision, accuracy, F-score, and AUC of a particular model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_scorer(model, features, labels, folds):\n",
    "    recall = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"recall\", n_jobs = -1))\n",
    "    precision = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"precision\", n_jobs = -1))\n",
    "    accuracy = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"accuracy\", n_jobs = -1))\n",
    "    f1 = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"f1\", n_jobs = -1))\n",
    "    auc = np.mean(cross_val_score(model, X = features, y = labels, cv = folds, scoring = \"roc_auc\", n_jobs = -1))\n",
    "    return accuracy, recall, precision, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir = \"/Users/Nick/Desktop/datafundamentals/thgfd/data/stratified \"\n",
    "files = glob.glob(file_dir + \"dataset*.csv\")\n",
    "data_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Processing and Modelling the data\n",
    "This is where the modelling happens, via a for loop. This whole process is automated by looping through a directory containing each of the datasets, producing a model, storing the model's metrics, writing to a .csv file and then moving onto the next file.\n",
    "\n",
    "#### Data read and validate\n",
    "The data is first read in, a variable of the file's name is created and then the data is checked for the presence of fraud. If this presence is confirmed, then the data is split into a testing and training set, with labels split out too. \n",
    "\n",
    "#### Modelling\n",
    "The training data is then passed through a random forest, using 80 trees. Once the model has been trained, variable importances are extracted and unimportant variables are then removed from the training and testing set. The model is then fitted again, using only the important variables.\n",
    "\n",
    "#### Testing\n",
    "The testing data is then passed through the model and, using cross validation, accuracy, precision, recall, f-score and auc are extracted. A confusion matrix and ROC plot are also created and saved to a .png file. The final model metrics and then written to csv and the loop proceeds to the next file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for file in files:\n",
    "    filename = file.strip(file_dir)\n",
    "    site = data_reader(file)\n",
    "    file_checks = file_checker(site)\n",
    "    if file_checks[1] > 0:\n",
    "        train, test = train_test_split(site, test_size = 0.5, random_state = 42)\n",
    "        train_x = train.loc[:, train.columns != \"fraud_status\"]\n",
    "        train_y = train['fraud_status']\n",
    "        test_x = test.loc[:, test.columns != \"fraud_status\"]\n",
    "        test_y = test['fraud_status']\n",
    "        classifier = RandomForestClassifier(80)\n",
    "        classifier.fit(train_x, train_y)\n",
    "        importances = pd.Series(classifier.feature_importances_, name = \"Importances\")\n",
    "        var_importance = pd.concat([pd.Series(test_x.columns, name = \"Names\"), importances], 1)\n",
    "        var_importance = var_importance.sort_values(by = \"Importances\", ascending = False).reset_index()\n",
    "        var_importance = var_importance.drop(\"index\", 1)\n",
    "        sns.set_style('ticks')\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(w = 20, h = 20)\n",
    "        sns.barplot(y = \"Names\", x = \"Importances\", data = var_importance, ax = ax)\n",
    "        sns.despine()\n",
    "        fig.savefig(\"/Users/sunmengnan/Desktop/FD/plot/\" +\n",
    "                    filename+ \"variable_importance_no_smote.png\")\n",
    "        \n",
    "        unimportant = var_importance.drop(var_importance[var_importance.Importances < 0.0001].index)\n",
    "        unimportant = unimportant[\"Names\"]\n",
    "        unimportant.to_csv(\"/Users/sunmengnan/Desktop/FD/result/\"+\n",
    "                           filename + \"important_variables.csv\",index = True)\n",
    "        \n",
    "        train_x = pd.DataFrame(train_x)\n",
    "        train_x.columns = train.loc[:, train.columns != \"fraud_status\"].columns\n",
    "        train_x = train_x[unimportant]\n",
    "        train_x = train_x.as_matrix()\n",
    "        test_x = test[unimportant].values\n",
    "        test_y = test['fraud_status'].values\n",
    "        \n",
    "        clf = RandomForestClassifier(80, n_jobs = -1, random_state=42)\n",
    "        clf.fit(train_x, train_y)\n",
    "        clf.predict(test_x)\n",
    "\n",
    "        if sum(test_y) < 10:\n",
    "            cv = sum(test_y)\n",
    "        else:\n",
    "            cv = 10\n",
    "        \n",
    "        accuracy, recall, precision, f_score, auc = data_scorer(clf, test_x, test_y, cv)\n",
    "        results = [filename, accuracy, recall, precision, f_score, auc]\n",
    "          \n",
    "        y_prob = clf.predict_proba(test_x)\n",
    "        y_pred = clf.predict(test_x)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(w = 20, h = 20)\n",
    "        skplt.metrics.plot_roc_curve(test_y, y_prob, title = \"Random Forest ROC Site \" + filename ,\n",
    "                                     ax = ax, title_fontsize=46, text_fontsize = 30)\n",
    "        fig.savefig(\"/Users/sunmengnan/Desktop/FD/plot/\" + filename + \"rf_roc_no_smote.png\")\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(w = 20, h = 20)\n",
    "        skplt.metrics.plot_confusion_matrix(test_y, y_pred, normalize = True, ax = ax,\n",
    "                                            text_fontsize=30, title_fontsize=46, \n",
    "                                            title=\"Normalised Confusion Matrix\")\n",
    "        \n",
    "        fig.savefig(\"/Users/sunmengnan/Desktop/FD/plot/\"+ filename + \"rf_confusion_no_smote.png\")\n",
    "        data_list.append(results)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results\n",
    "This saves all results into a .csv file called \"random_forest_results.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(data_list, columns = [\"File\",\"Accuracy\", \" recall\", \"precision\", \"f_score\", \"auc\"])\n",
    "results_df.to_csv(\"random_forest_results.csv\",\n",
    "                  index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
